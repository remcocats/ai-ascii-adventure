# AI ASCII Adventure

Turn-based text adventures powered by local LLMs, rendered in a Vaadin UI — no cloud required.

This project lets you play a lightweight D&D-style adventure where a Hero and NPCs take turns. The story is generated by
an LLM (via Spring AI + Ollama). The UI is built with Vaadin and features health/mana bars, inventory and weapons, and a
scrolling story view.

## Demo/Video

For background and live coding of the core ideas, see the live session: https://youtube.com/live/2P7NASv-LdE

## Tech Stack

- Java 21, Spring Boot 3.5
- Vaadin 24 (server-side UI with Push)
- Spring AI (Ollama chat model + chat memory)
- LangGraph4j (agent/executor integrations)
- Reactor streaming for incremental AI output
- Optional: Dockerized Ollama and Stable Diffusion WebUI (for experiments)

## Features (current)

- Hero setup dialog (name, race, class)
- LLM-generated, incremental story with choices
- NPC and hero turns/decisions (basic agent routing)
- Message-window chat memory for more coherent sessions
- Health/Mana/Spell Slots progress bars
- Inventory and weapons panels
- Streaming responses with graceful cancellation

## Roadmap (planned/experimental)

- Input validation (numbered choices only)
- ASCII art generation (experimental, model-dependent)
- Function-calling hooks to adjust health/mana/inventory
- Use items from inventory, spells, and effects
- Clear game goals and win/lose conditions

---

## Prerequisites

- Java 21
- Maven 3.9+
- Optional: Docker (to run Ollama locally with one command)
- GPU optional (Docker compose has a GPU profile; you can also run CPU-only)

## Quick Start

### 1) Start Ollama (recommended via Docker)

This repository includes a compose file that exposes Ollama on http://localhost:11434 and preloads a model.

- Start (GPU-friendly compose):
    - `cd Docker`
    - `docker compose up -d`
    - The init script runs `ollama serve` and pulls `llama3.2` on first run. You can change models later.

If you don't have a GPU, you may need to remove the `deploy.resources` GPU section from `Docker/docker-compose.yml` or
run Ollama natively:

- Install Ollama: https://ollama.com
- Then run: `ollama serve` (and in another terminal, `ollama run llama3.2`)

### 2) Configure the app (optional)

Default settings are defined in `src/main/resources/application.properties`:

- `server.port=8082` — App runs at http://localhost:8082
- `spring.ai.ollama.base-url=http://localhost:11434`
- `spring.ai.ollama.chat.model=gpt-oss:20b` (change to a model you have, e.g. `llama3.2`)
- Additional custom properties used in the app:
    - `ai.base-url=http://localhost:11434`
    - `ai.default-model=gpt-oss:20b`
    - `ai.vision-model=llava`
    - `ai.classification-model=llama3.2`
    - Temperatures and delays are also configurable.

You can override any of these with environment variables (Spring relaxed binding), for example:

- `SPRING_AI_OLLAMA_BASE_URL=http://localhost:11434`
- `SPRING_AI_OLLAMA_CHAT_MODEL=llama3.2`
- `AI_BASE_URL=http://localhost:11434`
- `AI_DEFAULT_MODEL=llama3.2`

### 3) Run the app

- From project root: `./mvnw spring-boot:run` (or `mvn spring-boot:run`)
- Open http://localhost:8082 in your browser.
- Enter your Hero details in the start dialog.
- The story will begin streaming in; choose actions to progress.

### Build a runnable JAR

- `./mvnw -DskipTests package`
- `java -jar target/ai-ascii-adventure-0.0.1-SNAPSHOT.jar`

## How it works (high level)

- UI: Vaadin SplitLayout shows the hero/NPC panels on the left and the story on the right.
- Streaming: Spring AI streams tokens; the UI updates as they arrive.
- Memory: A simple message-window chat memory provides short-term context.
- Routing: A lightweight classification prompt picks which “agent” (hero, npc, goblin) should act.
- Cancellation: In-flight requests are tracked and disposed when navigating away/shutting down.

Key classes to browse:

- `AiAsciiAdventureApplication` — Spring Boot + Vaadin configuration (Push enabled).
- `ChatView` — Main UI (hero setup, story, NPC panel, streaming).
- `AiService` — Chat client creation, streaming story/ascii, routing classifier.
- `config/*` — Model properties and Spring AI setup.
- `dto/*` — Hero/NPC/Story models.
- `tools/*` — UI helpers (hero/npc/story communicators, dice).

## Models and sizes

Defaults point to:

- Chat: `gpt-oss:20b` (change to a smaller model like `llama3.2` if needed)
- Vision (optional/experimental): `llava`
- Classification: `llama3.2`

If a model isn’t found locally, Ollama will try to pull it on first use (which can be several GB). You can pre-pull
models, for example:

- `ollama pull llama3.2`
- `ollama pull llava`

## Troubleshooting

- 404/connection refused to Ollama: ensure `ollama serve` is running on `localhost:11434`.
- Model not found or too heavy: set `SPRING_AI_OLLAMA_CHAT_MODEL=llama3.2` and/or `AI_DEFAULT_MODEL=llama3.2`.
- Slow first response: first run compiles the Vaadin dev bundle and may pull models.
- macOS Netty native transport warnings: the app disables native transports by default at startup.
- GPU issues in Docker: remove the `deploy.resources.devices` GPU section or run Ollama natively.

## Notes

- Port overview:
    - App: `8082`
    - Ollama: `11434`
    - Optional Stable Diffusion WebUI (from compose): `8081`, `7860`
- The Stable Diffusion service in `Docker/docker-compose.yml` is optional and provided for experimentation with
  image/ASCII ideas; the game itself does not require it.

## License

This repository is shared for educational/demo purposes. Add a license of your choice if you plan to distribute
binaries.
